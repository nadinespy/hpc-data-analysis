{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HPC Usage Statistics Visualisation\n\nThis notebook visualises HPC cluster usage statistics by faculty, with a focus on resource efficiency.\n\n**Sections:**\n1. [Overview](#1.-Overview) - Basic resource usage metrics by faculty\n2. [Global Stats](#2.-Global-Stats) - Key metrics at a glance\n3. [Detailed Analysis](#3.-Detailed-Analysis) - Comprehensive charts and comparisons\n4. [Correlation Analysis](#4.-Correlation-Analysis) - Relationships between metrics\n5. [Why Efficiency Matters](#5.-Why-Efficiency-Matters) - Educational content\n6. [Technical Appendix](#6.-Technical-Appendix) - Methodology and raw data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\nFACULTY_STATS_PATH = \"results/hpc_stats_output.csv\"\nJOB_LEVEL_PATH = \"results/job_level_metrics.csv\"\n\ndf = pd.read_csv(FACULTY_STATS_PATH)\ndf['faculty'] = df['faculty'].str.strip('\"')\ndf_faculties = df[df['faculty'] != 'all'].copy()\ndf_global = df[df['faculty'] == 'all'].copy()\n\n# Convert NULL strings to NaN\ndf_faculties = df_faculties.replace('NULL', np.nan)\nfor col in df_faculties.columns:\n    if col != 'faculty' and col != 'exit_codes':\n        df_faculties[col] = pd.to_numeric(df_faculties[col], errors='coerce')\n\n# Faculty name mapping: merge historically different names for the same faculty\nFACULTY_MERGE = {\n    \"Faculty of Life Sciences and Medicine\": \"Faculty of Life Sciences & Medicine\",\n    \"IoPPN\": \"Institute of Psychiatry, Psychology & Neuroscience\",\n    \"Dentistry, Oral & Craniofacial Sciences\": \"Faculty of Dentistry, Oral & Craniofacial Sciences\",\n    \"Social Science & Public Policy\": \"Faculty of Social Science & Public Policy\",\n}\n\ndf_faculties['faculty'] = df_faculties['faculty'].replace(FACULTY_MERGE)\n\n# --- Re-aggregate after faculty merge ---\n# Recover weighted-efficiency denominators before grouping.\n# weighted_eff = numerator / denominator * 100  =>  denominator = numerator * 100 / eff\ndf_faculties['_cpu_eff_denom'] = np.where(\n    df_faculties['weighted_cpu_eff_pct'] > 0,\n    df_faculties['total_cpu_sec'] * 100 / df_faculties['weighted_cpu_eff_pct'], 0)\ndf_faculties['_time_eff_denom'] = np.where(\n    df_faculties['weighted_time_eff_pct'] > 0,\n    df_faculties['total_elapsed_sec'] * 100 / df_faculties['weighted_time_eff_pct'], 0)\n\n# Columns to sum directly\nsum_cols = [\n    'job_count', 'job_count_success', 'job_count_failed',\n    'count_completed', 'count_cancelled', 'count_failed',\n    'count_timeout', 'count_node_fail', 'count_preempted',\n    'total_elapsed_sec', 'total_cpu_sec', 'total_user_cpu_sec', 'total_sys_cpu_sec',\n    'total_maxrss_bytes', 'total_reqmem_bytes',\n    'total_alloccpus', 'total_nodes', 'total_wait_sec',\n    '_cpu_eff_denom', '_time_eff_denom',\n]\n\n# For average efficiencies: pre-multiply by job count so we can compute\n# a weighted average after summing\navg_eff_cols = ['avg_cpu_eff_pct', 'avg_mem_eff_pct', 'avg_time_eff_pct']\nfor col in avg_eff_cols:\n    df_faculties[f'_{col}_wsum'] = df_faculties[col].fillna(0) * df_faculties['job_count']\n    sum_cols.append(f'_{col}_wsum')\n\n# Same for success-specific efficiencies (weighted by job_count_success)\nsuccess_cols = [\n    'success_weighted_cpu_eff_pct', 'success_avg_cpu_eff_pct',\n    'success_weighted_mem_eff_pct', 'success_avg_mem_eff_pct',\n    'success_weighted_time_eff_pct', 'success_avg_time_eff_pct',\n]\nfor col in success_cols:\n    df_faculties[f'_{col}_wsum'] = df_faculties[col].fillna(0) * df_faculties['job_count_success']\n    sum_cols.append(f'_{col}_wsum')\n\n# Merge exit_codes strings (combine code:count pairs)\ndef merge_exit_codes(series):\n    from collections import Counter\n    merged = Counter()\n    for s in series.dropna():\n        for pair in str(s).strip('\"').split(';'):\n            if ':' in pair:\n                code, count = pair.split(':')\n                merged[int(code)] += int(count)\n    if not merged:\n        return ''\n    return ';'.join(f'{k}:{v}' for k, v in sorted(merged.items()))\n\nexit_codes_merged = df_faculties.groupby('faculty')['exit_codes'].apply(merge_exit_codes)\n\n# Group and sum\ndf_faculties = df_faculties.groupby('faculty')[sum_cols].sum().reset_index()\n\n# Recompute averages from totals\ndf_faculties['avg_elapsed_sec'] = df_faculties['total_elapsed_sec'] / df_faculties['job_count']\ndf_faculties['avg_cpu_sec'] = df_faculties['total_cpu_sec'] / df_faculties['job_count']\ndf_faculties['avg_maxrss_bytes'] = df_faculties['total_maxrss_bytes'] / df_faculties['job_count']\ndf_faculties['avg_reqmem_bytes'] = df_faculties['total_reqmem_bytes'] / df_faculties['job_count']\ndf_faculties['avg_alloccpus'] = df_faculties['total_alloccpus'] / df_faculties['job_count']\ndf_faculties['avg_wait_sec'] = df_faculties['total_wait_sec'] / df_faculties['job_count']\ndf_faculties['user_cpu_pct'] = (df_faculties['total_user_cpu_sec'] /\n    (df_faculties['total_user_cpu_sec'] + df_faculties['total_sys_cpu_sec']) * 100)\ndf_faculties['sys_cpu_pct'] = 100 - df_faculties['user_cpu_pct']\n\n# Recompute weighted efficiencies from recovered denominators\ndf_faculties['weighted_cpu_eff_pct'] = np.where(\n    df_faculties['_cpu_eff_denom'] > 0,\n    df_faculties['total_cpu_sec'] / df_faculties['_cpu_eff_denom'] * 100, np.nan)\ndf_faculties['weighted_mem_eff_pct'] = np.where(\n    df_faculties['total_reqmem_bytes'] > 0,\n    df_faculties['total_maxrss_bytes'] / df_faculties['total_reqmem_bytes'] * 100, np.nan)\ndf_faculties['weighted_time_eff_pct'] = np.where(\n    df_faculties['_time_eff_denom'] > 0,\n    df_faculties['total_elapsed_sec'] / df_faculties['_time_eff_denom'] * 100, np.nan)\n\n# Average efficiencies (job-count-weighted average)\nfor col in avg_eff_cols:\n    df_faculties[col] = df_faculties[f'_{col}_wsum'] / df_faculties['job_count']\n\n# Success efficiencies (job_count_success-weighted average)\nfor col in success_cols:\n    df_faculties[col] = np.where(\n        df_faculties['job_count_success'] > 0,\n        df_faculties[f'_{col}_wsum'] / df_faculties['job_count_success'], np.nan)\n\n# Re-attach exit_codes\ndf_faculties = df_faculties.merge(exit_codes_merged.rename('exit_codes'), on='faculty', how='left')\n\n# Drop temporary columns\ndf_faculties = df_faculties.drop(columns=[c for c in df_faculties.columns if c.startswith('_')])\n\n# Sorted view for bar plots\ndf_plot = df_faculties.sort_values('job_count', ascending=True)\n\nprint(f\"Loaded and merged aggregate data: {len(df_faculties)} faculties\")\nprint(f\"Total jobs: {df_faculties['job_count'].sum():,}\")\nprint(f\"Faculties: {', '.join(sorted(df_faculties['faculty']))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Shortened faculty names for plot labels\nimport textwrap\n\ndef _shorten_faculty(name):\n    s = name.replace(\"Faculty of \", \"\").replace(\"Institute of \", \"\")\n    return '\\n'.join(textwrap.wrap(s, width=25))\n\ndf_faculties['faculty_short'] = df_faculties['faculty'].apply(_shorten_faculty)\nFACULTY_SHORT = dict(zip(df_faculties['faculty'], df_faculties['faculty_short']))\ndf_plot = df_faculties.sort_values('job_count', ascending=True)\n\n# Try to load per-job data (optional, for distribution analysis)\ntry:\n    df_jobs = pd.read_csv(JOB_LEVEL_PATH)\n    if 'faculty' in df_jobs.columns:\n        df_jobs['faculty'] = df_jobs['faculty'].str.strip('\"')\n        df_jobs['faculty'] = df_jobs['faculty'].replace(FACULTY_MERGE)\n        df_jobs['faculty_short'] = df_jobs['faculty'].map(FACULTY_SHORT).fillna(df_jobs['faculty'])\n    df_jobs = df_jobs.replace('NULL', np.nan)\n    for col in ['cpu_eff_pct', 'mem_eff_pct', 'time_eff_pct', 'wait_sec', 'elapsed_sec']:\n        if col in df_jobs.columns:\n            df_jobs[col] = pd.to_numeric(df_jobs[col], errors='coerce')\n    HAS_JOB_DATA = True\n    print(f\"Loaded per-job data: {len(df_jobs):,} jobs\")\nexcept FileNotFoundError:\n    HAS_JOB_DATA = False\n    print(f\"Note: {JOB_LEVEL_PATH} not found. Distribution analysis will be skipped.\")\n    print(\"Generate it with: python3 job_level_metrics.py --include-faculty --output results/job_level_metrics.csv ...\")"
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 1. Overview\n\nBasic resource usage metrics by faculty. Faculties with historically different names have been merged (e.g. \"IoPPN\" and \"Institute of Psychiatry, Psychology & Neuroscience\").",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "BAR_COLOR = '#4d4d4d'\n\ndef overview_bar(values, labels, xlabel):\n    \"\"\"Simple horizontal bar chart for overview metrics.\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.barh(labels, values, color=BAR_COLOR)\n    ax.set_xlabel(xlabel)\n    plt.tight_layout()\n    plt.show()\n\n# Total job count\noverview_bar(df_plot['job_count'], df_plot['faculty_short'], 'Total jobs')\n\n# Total job duration\noverview_bar(df_plot['total_elapsed_sec'] / 86400, df_plot['faculty_short'],\n             'Total job duration (days)')\n\n# Mean job duration\noverview_bar(df_plot['avg_elapsed_sec'] / 3600, df_plot['faculty_short'],\n             'Mean job duration (hours)')\n\n# Mean number of CPUs requested\noverview_bar(df_plot['avg_alloccpus'], df_plot['faculty_short'],\n             'Mean number of CPUs requested')\n\n# Total CPU time\noverview_bar(df_plot['total_cpu_sec'] / 86400, df_plot['faculty_short'],\n             'Total CPU time (days)')\n\n# Mean CPU time\noverview_bar(df_plot['avg_cpu_sec'] / 3600, df_plot['faculty_short'],\n             'Mean CPU time (hours)')\n\n# Mean memory requested\noverview_bar(df_plot['avg_reqmem_bytes'] / 1e9, df_plot['faculty_short'],\n             'Mean memory requested (GB)')\n\n# Mean memory used\noverview_bar(df_plot['avg_maxrss_bytes'] / 1e9, df_plot['faculty_short'],\n             'Mean memory used (GB)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 2. Global Stats\n\nKey metrics for quick decision-making."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary():\n",
    "    \"\"\"Display summary statistics.\"\"\"\n",
    "    total_jobs = df_faculties['job_count'].sum()\n",
    "    total_success = df_faculties['job_count_success'].sum()\n",
    "    total_failed = df_faculties['job_count_failed'].sum()\n",
    "    success_rate = (total_success / total_jobs * 100) if total_jobs > 0 else 0\n",
    "    \n",
    "    if len(df_global) > 0:\n",
    "        global_cpu_eff = df_global['weighted_cpu_eff_pct'].values[0]\n",
    "        global_mem_eff = df_global['weighted_mem_eff_pct'].values[0]\n",
    "    else:\n",
    "        global_cpu_eff = df_faculties['weighted_cpu_eff_pct'].mean()\n",
    "        global_mem_eff = df_faculties['weighted_mem_eff_pct'].mean()\n",
    "    \n",
    "    avg_wait = df_faculties['avg_wait_sec'].mean() / 60\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Total Jobs** | {total_jobs:,} |\n",
    "| **Success Rate** | {success_rate:.1f}% |\n",
    "| **CPU Efficiency** (weighted) | {global_cpu_eff:.1f}% |\n",
    "| **Memory Efficiency** (weighted) | {global_mem_eff:.1f}% |\n",
    "| **Avg Queue Wait** | {avg_wait:.1f} min |\n",
    "\"\"\"\n",
    "    display(Markdown(summary))\n",
    "\n",
    "create_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def efficiency_ranking():\n    \"\"\"Display faculty efficiency ranking (sorted by weighted CPU efficiency).\"\"\"\n    ranking = df_faculties[['faculty', 'job_count', 'weighted_cpu_eff_pct', \n                            'weighted_mem_eff_pct', 'job_count_failed']].copy()\n    ranking['failure_rate'] = (ranking['job_count_failed'] / ranking['job_count'] * 100).round(1)\n    ranking = ranking.sort_values('weighted_cpu_eff_pct', ascending=False)\n    \n    display(Markdown(\"### Faculty Ranking by Weighted CPU Efficiency\"))\n    display(ranking[['faculty', 'job_count', 'weighted_cpu_eff_pct', \n                     'weighted_mem_eff_pct', 'failure_rate']].rename(columns={\n        'faculty': 'Faculty', 'job_count': 'Jobs', 'weighted_cpu_eff_pct': 'CPU Eff %',\n        'weighted_mem_eff_pct': 'Mem Eff %', 'failure_rate': 'Failure %'\n    }))\n\nefficiency_ranking()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 3. Detailed Analysis\n\nComprehensive visualisations for in-depth analysis.\n\n## 3.1 Job Counts by Faculty"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_job_counts():\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    sorted_df = df_faculties.sort_values('job_count', ascending=True)\n    \n    axes[0].barh(sorted_df['faculty_short'], sorted_df['job_count'], color='steelblue')\n    axes[0].set_xlabel('Number of Jobs')\n    axes[0].set_title('Total Jobs by Faculty')\n    \n    axes[1].barh(sorted_df['faculty_short'], sorted_df['job_count_success'], \n                 label='Successful', color='#4caf50', alpha=0.8)\n    axes[1].barh(sorted_df['faculty_short'], sorted_df['job_count_failed'], \n                 left=sorted_df['job_count_success'], label='Failed', color='#f44336', alpha=0.8)\n    axes[1].set_xlabel('Number of Jobs')\n    axes[1].set_title('Job Success vs Failure by Faculty')\n    axes[1].legend(loc='lower right')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_job_counts()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3.2 Efficiency Comparisons"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_efficiency_comparison():\n    # CPU efficiency: weighted vs average (side by side, ranked)\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    sorted_df = df_faculties.sort_values('weighted_cpu_eff_pct', ascending=True)\n    axes[0].barh(sorted_df['faculty_short'], sorted_df['weighted_cpu_eff_pct'], color='#2196f3')\n    axes[0].set_xlabel('Efficiency (%)')\n    axes[0].set_title('Weighted CPU Efficiency')\n    axes[0].axvline(x=100, color='green', linestyle='--', alpha=0.5)\n    \n    sorted_df = df_faculties.sort_values('avg_cpu_eff_pct', ascending=True)\n    axes[1].barh(sorted_df['faculty_short'], sorted_df['avg_cpu_eff_pct'], color='#90caf9')\n    axes[1].set_xlabel('Efficiency (%)')\n    axes[1].set_title('Average CPU Efficiency')\n    axes[1].axvline(x=100, color='green', linestyle='--', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Memory efficiency: weighted vs average (side by side, ranked)\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    sorted_df = df_faculties.sort_values('weighted_mem_eff_pct', ascending=True)\n    axes[0].barh(sorted_df['faculty_short'], sorted_df['weighted_mem_eff_pct'], color='#4caf50')\n    axes[0].set_xlabel('Efficiency (%)')\n    axes[0].set_title('Weighted Memory Efficiency')\n    axes[0].axvline(x=100, color='green', linestyle='--', alpha=0.5)\n    \n    sorted_df = df_faculties.sort_values('avg_mem_eff_pct', ascending=True)\n    axes[1].barh(sorted_df['faculty_short'], sorted_df['avg_mem_eff_pct'], color='#a5d6a7')\n    axes[1].set_xlabel('Efficiency (%)')\n    axes[1].set_title('Average Memory Efficiency')\n    axes[1].axvline(x=100, color='green', linestyle='--', alpha=0.5)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_efficiency_comparison()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3.3 Wait Time Analysis"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def plot_wait_time():\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    sorted_df = df_faculties.sort_values('total_wait_sec', ascending=True)\n    axes[0].barh(sorted_df['faculty_short'], sorted_df['total_wait_sec'] / 3600,\n                 color='steelblue')\n    axes[0].set_xlabel('Total Wait Time (hours)')\n    \n    sorted_df = df_faculties.sort_values('avg_wait_sec', ascending=True)\n    axes[1].barh(sorted_df['faculty_short'], sorted_df['avg_wait_sec'] / 60,\n                 color='#ff9800')\n    axes[1].set_xlabel('Average Wait Time (minutes)')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_wait_time()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 3.4 Job-level Analysis\n\nPer-job efficiency distributions using violin plots. This shows variance and outliers within each faculty. All violins are scaled to the same maximum width (`density_norm='width'`), so their shapes show the *relative* distribution within each faculty but their widths are not comparable across faculties.\n\n**Note:** This section requires the per-job data file generated by:\n```bash\npython3 job_level_metrics.py --include-faculty --since 2025-01-01 --until 2025-02-01 --output results/job_level_metrics.csv\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_violin_cpu_efficiency():\n    \"\"\"Violin plot of CPU efficiency distribution by faculty.\"\"\"\n    if not HAS_JOB_DATA:\n        print(\"Per-job data not available. Run job_level_metrics.py first.\")\n        return\n    \n    if 'faculty' not in df_jobs.columns:\n        print(\"Faculty column not in job data. Run job_level_metrics.py with --include-faculty.\")\n        return\n    \n    plot_data = df_jobs[df_jobs['cpu_eff_pct'].notna() & (df_jobs['cpu_eff_pct'] <= 200)].copy()\n    top_faculties = plot_data['faculty'].value_counts().head(10).index.tolist()\n    plot_data = plot_data[plot_data['faculty'].isin(top_faculties)]\n    \n    # Use same ranking as weighted efficiency bar plots\n    ranking = (df_faculties[df_faculties['faculty'].isin(top_faculties)]\n               .sort_values('weighted_cpu_eff_pct'))\n    order = [FACULTY_SHORT.get(f, f) for f in ranking['faculty']]\n    \n    fig, ax = plt.subplots(figsize=(12, 8))\n    sns.violinplot(data=plot_data, y='faculty_short', x='cpu_eff_pct', ax=ax,\n                   cut=0, order=order, density_norm='width', inner='quartile')\n    ax.axvline(x=100, color='green', linestyle='--', alpha=0.5, label='100% (optimal)')\n    ax.axvline(x=50, color='orange', linestyle='--', alpha=0.5, label='50% threshold')\n    ax.set_ylabel('')\n    ax.set_xlabel('CPU Efficiency (%)')\n    ax.set_title('CPU Efficiency Distribution by Faculty')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\nplot_violin_cpu_efficiency()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_violin_memory_efficiency():\n    \"\"\"Violin plot of memory efficiency distribution by faculty.\"\"\"\n    if not HAS_JOB_DATA:\n        print(\"Per-job data not available. Run job_level_metrics.py first.\")\n        return\n    \n    if 'faculty' not in df_jobs.columns:\n        print(\"Faculty column not in job data. Run job_level_metrics.py with --include-faculty.\")\n        return\n    \n    plot_data = df_jobs[df_jobs['mem_eff_pct'].notna() & (df_jobs['mem_eff_pct'] <= 200)].copy()\n    top_faculties = plot_data['faculty'].value_counts().head(10).index.tolist()\n    plot_data = plot_data[plot_data['faculty'].isin(top_faculties)]\n    \n    # Use same ranking as weighted efficiency bar plots\n    ranking = (df_faculties[df_faculties['faculty'].isin(top_faculties)]\n               .sort_values('weighted_mem_eff_pct'))\n    order = [FACULTY_SHORT.get(f, f) for f in ranking['faculty']]\n    \n    fig, ax = plt.subplots(figsize=(12, 8))\n    sns.violinplot(data=plot_data, y='faculty_short', x='mem_eff_pct', ax=ax,\n                   cut=0, order=order, density_norm='width', inner='quartile')\n    ax.axvline(x=100, color='green', linestyle='--', alpha=0.5, label='100% (optimal)')\n    ax.axvline(x=50, color='orange', linestyle='--', alpha=0.5, label='50% threshold')\n    ax.set_ylabel('')\n    ax.set_xlabel('Memory Efficiency (%)')\n    ax.set_title('Memory Efficiency Distribution by Faculty')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\nplot_violin_memory_efficiency()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def plot_box_comparison():\n    \"\"\"Box plots comparing CPU and memory efficiency side by side.\"\"\"\n    if not HAS_JOB_DATA:\n        print(\"Per-job data not available.\")\n        return\n    \n    if 'faculty' not in df_jobs.columns:\n        print(\"Faculty column not in job data.\")\n        return\n    \n    top_faculties = df_jobs['faculty'].value_counts().head(8).index.tolist()\n    plot_data = df_jobs[df_jobs['faculty'].isin(top_faculties)].copy()\n    \n    # Use same ranking as weighted efficiency bar plots (by weighted CPU efficiency)\n    ranking = (df_faculties[df_faculties['faculty'].isin(top_faculties)]\n               .sort_values('weighted_cpu_eff_pct'))\n    order = [FACULTY_SHORT.get(f, f) for f in ranking['faculty']]\n    \n    melt_data = plot_data[['faculty_short', 'cpu_eff_pct', 'mem_eff_pct']].melt(\n        id_vars='faculty_short', value_vars=['cpu_eff_pct', 'mem_eff_pct'],\n        var_name='Metric', value_name='Efficiency %'\n    )\n    melt_data = melt_data[melt_data['Efficiency %'].notna() & (melt_data['Efficiency %'] <= 200)]\n    melt_data['Metric'] = melt_data['Metric'].map({'cpu_eff_pct': 'CPU', 'mem_eff_pct': 'Memory'})\n    \n    fig, ax = plt.subplots(figsize=(12, 8))\n    sns.boxplot(data=melt_data, y='faculty_short', x='Efficiency %', hue='Metric',\n                ax=ax, order=order)\n    ax.axvline(x=100, color='green', linestyle='--', alpha=0.5)\n    ax.set_ylabel('')\n    ax.set_title('CPU vs Memory Efficiency Distribution by Faculty')\n    plt.tight_layout()\n    plt.show()\n\nplot_box_comparison()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 4. Correlation Analysis\n\nInvestigating relationships between per-job metrics. We use per-job data (from `job_level_metrics.csv`) rather than faculty-level aggregates to ensure sufficient sample size for meaningful statistics. We first check normality to determine the appropriate correlation method.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_normality(data, name):\n    \"\"\"Test for normality using Shapiro-Wilk test.\"\"\"\n    clean_data = data.dropna()\n    if len(clean_data) < 3:\n        return None, None\n    # Shapiro-Wilk has a limit of 5000 samples; use a random subsample if larger\n    if len(clean_data) > 5000:\n        clean_data = clean_data.sample(5000, random_state=42)\n    stat, p_value = stats.shapiro(clean_data)\n    return stat, p_value\n\ndef normality_report():\n    \"\"\"Report on normality of key per-job metrics.\"\"\"\n    if not HAS_JOB_DATA:\n        print(\"Per-job data not available. Run job_level_metrics.py first.\")\n        return\n\n    metrics = ['cpu_eff_pct', 'mem_eff_pct', 'time_eff_pct', 'wait_sec', 'elapsed_sec']\n    metrics = [m for m in metrics if m in df_jobs.columns]\n\n    results = []\n    for metric in metrics:\n        stat, p = test_normality(df_jobs[metric], metric)\n        if p is not None:\n            normal = \"Yes\" if p > 0.05 else \"No\"\n            results.append(f\"| {metric} | {stat:.3f} | {p:.4f} | {normal} |\")\n\n    n = len(df_jobs)\n    report = f\"\"\"### Normality Tests (Shapiro-Wilk) on Per-Job Data\n\nSample size: **{n:,}** jobs (subsampled to 5,000 if larger for Shapiro-Wilk)\n\n| Metric | Statistic | p-value | Normal? (p>0.05) |\n|--------|-----------|---------|------------------|\n\"\"\" + \"\\n\".join(results) + \"\"\"\n\n**Interpretation:** If data is not normally distributed, we use Spearman correlation (rank-based) instead of Pearson.\n\"\"\"\n    display(Markdown(report))\n\nnormality_report()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def correlation_analysis():\n    \"\"\"Compute and visualise correlation matrix using per-job data.\"\"\"\n    if not HAS_JOB_DATA:\n        print(\"Per-job data not available. Run job_level_metrics.py first.\")\n        return\n\n    nice_names = {\n        'cpu_eff_pct': 'CPU Efficiency (%)',\n        'mem_eff_pct': 'Memory Efficiency (%)',\n        'time_eff_pct': 'Time Efficiency (%)',\n        'wait_sec': 'Wait Time (s)',\n        'elapsed_sec': 'Job Duration (s)',\n    }\n\n    corr_cols = [c for c in nice_names if c in df_jobs.columns]\n    corr_data = df_jobs[corr_cols].dropna()\n    corr_data = corr_data.rename(columns=nice_names)\n\n    display(Markdown(f\"Correlation computed on **{len(corr_data):,}** jobs with complete data.\"))\n\n    # Use Spearman (more robust for non-normal data)\n    corr_matrix = corr_data.corr(method='spearman')\n\n    fig, ax = plt.subplots(figsize=(10, 8))\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n                center=0, square=True, linewidths=0.5, ax=ax)\n    ax.set_title('Correlation Matrix (Spearman)', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n    cpu_name = nice_names.get('cpu_eff_pct')\n    mem_name = nice_names.get('mem_eff_pct')\n    if cpu_name in corr_matrix.columns and mem_name in corr_matrix.columns:\n        valid = corr_data[[cpu_name, mem_name]].dropna()\n        rho, pval = stats.spearmanr(valid[cpu_name], valid[mem_name])\n        display(Markdown(\n            f\"**CPU vs Memory Efficiency:** r = {rho:.3f}, p = {pval:.2e} \"\n            f\"(n = {len(valid):,})\"\n        ))\n\ncorrelation_analysis()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def cpu_memory_scatter():\n    \"\"\"Scatter plot: Does poor CPU efficiency coincide with poor memory efficiency?\"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    plot_df = df_faculties.dropna(subset=['weighted_cpu_eff_pct', 'weighted_mem_eff_pct'])\n    \n    ax.scatter(plot_df['weighted_mem_eff_pct'], plot_df['weighted_cpu_eff_pct'],\n               s=plot_df['job_count']/plot_df['job_count'].max()*500+50,\n               alpha=0.6, c=range(len(plot_df)), cmap='tab10')\n    \n    for idx, row in plot_df.iterrows():\n        ax.annotate(row['faculty_short'].replace('\\n', ' '),\n                    (row['weighted_mem_eff_pct'], row['weighted_cpu_eff_pct']),\n                   fontsize=8, alpha=0.8)\n    \n    ax.axhline(y=50, color='orange', linestyle='--', alpha=0.5, label='50% threshold')\n    ax.axhline(y=100, color='green', linestyle='--', alpha=0.5, label='100% (optimal)')\n    ax.axvline(x=50, color='orange', linestyle=':', alpha=0.5)\n    ax.axvline(x=100, color='green', linestyle=':', alpha=0.5)\n    \n    # Add regression line\n    z = np.polyfit(plot_df['weighted_mem_eff_pct'], plot_df['weighted_cpu_eff_pct'], 1)\n    p = np.poly1d(z)\n    x_line = np.linspace(plot_df['weighted_mem_eff_pct'].min(), plot_df['weighted_mem_eff_pct'].max(), 100)\n    ax.plot(x_line, p(x_line), 'r--', alpha=0.5, label='Trend line')\n    \n    ax.set_xlabel('Memory Efficiency (%)')\n    ax.set_ylabel('CPU Efficiency (%)')\n    ax.set_title('CPU vs Memory Efficiency by Faculty (bubble size = job count)')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\ncpu_memory_scatter()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 5. Why Efficiency Matters\n\n## The Problem: Resource Waste\n\nWhen you request resources (CPUs, memory) for your job, those resources are **reserved exclusively for you**, even if you don't use them. This means:\n\n- Other users can't run their jobs while your resources sit idle\n- Queue wait times increase for everyone\n- The cluster appears \"full\" when it's actually underutilised\n\n## Example: Memory Over-requesting\n\nIf you request **64 GB** of memory but only use **2 GB**:\n- Efficiency: 2/64 = **3.1%**\n- Wasted: 62 GB that could have run **31 other 2GB jobs**\n\n## What \"Good\" Efficiency Looks Like\n\n| Range | Rating | Meaning |\n|-------|--------|---------|\n| 70–100% | Excellent | You're using what you requested |\n| 30–70% | Acceptable | Room for improvement |\n| <30% | Poor | Significant waste — please optimise |\n| >100% | See note | Not necessarily a problem (see below) |\n\n**Efficiency >100%** has different causes for CPU and memory:\n- **CPU >100%**: The job used more CPU time than `elapsed × requested CPUs`. This typically happens when code spawns threads internally (e.g., OpenMP) without Slurm being aware — Slurm cannot track the actual thread count. It does not mean the user did anything wrong.\n- **Memory >100%**: The job used more memory than it requested. This can happen if memory limits are not enforced by the cluster (see Technical Appendix). The job was not killed because Slurm's memory enforcement is a configuration option, not a default.\n\n## How to Improve Your Efficiency\n\n1. **Profile your jobs first** — Run a small test job and check actual resource usage with `seff <jobid>`\n2. **Request what you need (plus 10–20% buffer)** — If your test used 8 GB, request 10 GB, not 64 GB \"just in case\"\n3. **Use job arrays for many small jobs** — Instead of requesting 100 CPUs for one job, submit 100 single-CPU jobs\n4. **Check your code's parallelisation** — If you request 16 CPUs but your code only uses 1, you're wasting 15 CPUs\n5. **Attend HPC training** — Learn best practices for efficient cluster usage"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_waste():\n    \"\"\"Visualise resource waste.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    df_faculties['mem_wasted_tb'] = ((df_faculties['total_reqmem_bytes'] - \n                                       df_faculties['total_maxrss_bytes']) / 1e12).clip(lower=0)\n    \n    sorted_df = df_faculties.sort_values('mem_wasted_tb', ascending=True)\n    \n    axes[0].barh(sorted_df['faculty_short'], sorted_df['mem_wasted_tb'], color='#f44336', alpha=0.7)\n    axes[0].set_xlabel('Wasted Memory (TB)')\n    axes[0].set_title('Memory Waste by Faculty (Requested - Used)')\n    \n    total_requested = df_faculties['total_reqmem_bytes'].sum() / 1e12\n    total_used = df_faculties['total_maxrss_bytes'].sum() / 1e12\n    total_wasted = max(0, total_requested - total_used)\n    \n    axes[1].pie([total_used, total_wasted], \n                labels=[f'Used\\n{total_used:.1f} TB', f'Wasted\\n{total_wasted:.1f} TB'],\n                colors=['#4caf50', '#f44336'], autopct='%1.1f%%', \n                startangle=90, explode=(0, 0.1))\n    axes[1].set_title('Cluster-Wide Memory Utilisation')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_waste()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 6. Technical Appendix\n\n## Data Sources\n\n- **Job data:** Slurm accounting database (MySQL), accessed directly via `mysql.connector` (not via `sacct`, for reliability).\n  - `create_job_table`: job metadata — timestamps, resource requests (`cpus_req`, `tres_req`), state, exit code, time limit\n  - `create_step_table`: per-step CPU time (`user_sec`, `sys_sec`) and peak memory usage (`tres_usage_in_max`)\n  - `create_assoc_table`: maps job associations to usernames\n- **Faculty mapping:** Active Directory via LDAP. Each username is looked up to retrieve its faculty attribute (`st`).\n\n## Efficiency Metrics\n\n### Weighted vs Average\n\nWe compute two versions of each efficiency metric:\n\n| Version | Formula | Question it answers |\n|---------|---------|---------------------|\n| **Weighted** | Sum(used) / Sum(allocated) × 100 | \"How efficiently are allocated resources being used overall?\" Larger jobs contribute more. |\n| **Average** | Mean(per-job efficiency) | \"What's the typical job's efficiency?\" Each job counts equally. |\n\n**Example** — CPU efficiency for a faculty with 2 jobs:\n\n| Job | TotalCPU | Elapsed | CPUs | Job Efficiency |\n|-----|----------|---------|------|----------------|\n| A   | 90s      | 100s    | 1    | 90%            |\n| B   | 1800s    | 10000s  | 2    | 9%             |\n\n- **Weighted:** (90 + 1800) / (100×1 + 10000×2) = 1890 / 20100 = **9.4%** — dominated by Job B, which used far more resources\n- **Average:** (90% + 9%) / 2 = **49.5%** — each job counts equally\n\n### CPU Efficiency\n\n`CPU Efficiency = TotalCPU / (Elapsed × CPUs Requested) × 100`\n\n- **TotalCPU** = user-mode CPU seconds + system-mode CPU seconds + microsecond fractions. This is summed across all computation steps of the job (see \"CPU time and step handling\" below).\n- **CPUs Requested** = `cpus_req` from the job table.\n\n**Why it can exceed 100%:** Slurm tracks CPU time via kernel accounting but does not track how many hardware threads a process uses. A program can spawn threads internally (e.g., OpenMP, Python multiprocessing) without Slurm knowing. If a job requests 1 CPU but runs 4 threads, TotalCPU can be ~4× the elapsed time, giving ~400% efficiency. There is no way to detect the actual thread count from the accounting database.\n\n### Memory Efficiency\n\n`Memory Efficiency = MaxRSS / ReqMem × 100`\n\n- **MaxRSS**: Peak memory usage, extracted from `tres_usage_in_max` in the step table (TRES ID 2). The numeric maximum across all steps is used.\n- **ReqMem**: Requested memory, extracted from `tres_req` in the job table (TRES ID 2), in MB. This is always stored as total memory regardless of whether the user specified `--mem` (per-node) or `--mem-per-cpu` — Slurm multiplies per-cpu values by the CPU count before storing.\n\n**Why it can exceed 100%:** Memory limits are **not enforced** on this cluster. A diagnostic found 2,082,500 jobs with memory efficiency >100%, all using per-node memory requests. Extreme cases include 100-CPU jobs requesting only 1 GB (likely a Slurm default) but using 60–70 GB (6000%+ efficiency). Memory enforcement requires the `cgroup` plugin with `ConstrainRAMSpace=yes` in `cgroup.conf`; without it, jobs can exceed their requested memory without being killed.\n\n### Time Efficiency\n\n`Time Efficiency = Elapsed / (Timelimit × 60) × 100`\n\nMeasures what fraction of the requested wall-clock time was actually used. Timelimit is stored in minutes in the database.\n\n### Wait Time\n\n`Wait Time = time_start − time_submit`\n\nHow long a job sat in the queue before starting. Shown as both total (sum across all jobs in a faculty) and average (typical queue experience).\n\n## CPU Time and Step Handling\n\nSlurm records CPU time per step within each job. There are two kinds of steps:\n\n- **Batch step**: the shell process running the job script (always exists)\n- **Regular steps** (0, 1, 2, ...): created by `srun` calls within the script\n\nFor jobs using `srun`, the batch step records only shell overhead (0–2 seconds of CPU), while the srun steps carry the actual computation. Summing all steps would double-count. The SQL query therefore sums only regular steps, falling back to the batch step for jobs that have no `srun` steps.\n\nStep IDs for batch and other internal steps (like `interactive`) are discovered dynamically at startup rather than hardcoded, for portability across Slurm versions.\n\n## MaxRSS Extraction\n\nMemory usage is stored in TRES (Trackable RESources) strings like `\"1=500,2=12345678,4=1\"`, where each ID=value pair represents a resource type (1=CPU, 2=memory, 4=nodes). To get MaxRSS, the memory value (TRES ID 2) is extracted numerically in SQL using `SUBSTRING_INDEX`, cast to an unsigned integer, and then the numeric `MAX` is taken across steps. This avoids a pitfall where `MAX()` on the raw string would do lexicographic comparison.\n\n## Job States Included\n\nOnly finished jobs are analysed: COMPLETED (3), CANCELLED (4), FAILED (5), TIMEOUT (6), NODE_FAIL (7), PREEMPTED (8). These codes are from Slurm's standard state definitions (`slurm.h`).\n\n## Faculty Merging\n\nFour pairs of historically different names for the same faculty are merged before analysis:\n\n| Old name | Merged into |\n|----------|-------------|\n| Faculty of Life Sciences and Medicine | Faculty of Life Sciences & Medicine |\n| IoPPN | Institute of Psychiatry, Psychology & Neuroscience |\n| Dentistry, Oral & Craniofacial Sciences | Faculty of Dentistry, Oral & Craniofacial Sciences |\n| Social Science & Public Policy | Faculty of Social Science & Public Policy |\n\n## Open Questions\n\n1. **CPU efficiency denominator**: Currently uses `cpus_req` (what the user requested). A diagnostic showed that 5.5% of jobs have more CPUs *allocated* than requested (most commonly 1→2, likely due to hyperthreading at the core level). Using allocated CPUs instead would lower efficiency for those jobs, but the extra allocation is outside the user's control.\n2. **MaxRSS units**: The code treats `tres_usage_in_max` TRES ID 2 values as bytes. This should be verified against `sacct` output for the same jobs."
  },
  {
   "cell_type": "code",
   "source": "def draw_db_schema():\n    \"\"\"Draw the database schema showing how Slurm tables and LDAP relate.\"\"\"\n    fig, ax = plt.subplots(figsize=(14, 8))\n    ax.set_xlim(0, 14)\n    ax.set_ylim(0, 10)\n    ax.axis('off')\n    ax.set_title('Data Schema: Slurm Accounting Database + LDAP',\n                 fontsize=14, fontweight='bold', pad=20)\n\n    def draw_table(x, y, w, h, title, columns, color='#e3f2fd', border='#1565c0'):\n        from matplotlib.patches import FancyBboxPatch\n        box = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.1\",\n                             facecolor=color, edgecolor=border, linewidth=1.5)\n        ax.add_patch(box)\n        ax.text(x + w / 2, y + h - 0.3, title, ha='center', va='top',\n                fontsize=11, fontweight='bold', color=border)\n        ax.plot([x + 0.15, x + w - 0.15], [y + h - 0.5, y + h - 0.5],\n                color=border, linewidth=0.5)\n        for i, (col, style) in enumerate(columns):\n            fw = 'bold' if style == 'pk' else 'normal'\n            fs = 'italic' if style == 'fk' else 'normal'\n            prefix = ''\n            if style == 'pk':\n                prefix = 'PK  '\n            elif style == 'fk':\n                prefix = 'FK  '\n            ax.text(x + 0.25, y + h - 0.85 - i * 0.35, prefix + col,\n                    ha='left', va='top', fontsize=9, fontweight=fw, fontstyle=fs)\n\n    # Job table (centre-left)\n    draw_table(0.5, 4, 4.5, 4.5, 'Job Table (create_job_table)', [\n        ('job_db_inx', 'pk'),\n        ('id_job', ''),\n        ('id_assoc', 'fk'),\n        ('state, exit_code', ''),\n        ('time_submit, time_start, time_end', ''),\n        ('cpus_req, tres_req', ''),\n        ('timelimit, nodes_alloc', ''),\n    ])\n\n    # Step table (right)\n    draw_table(9, 5, 4.5, 3.5, 'Step Table (create_step_table)', [\n        ('job_db_inx', 'fk'),\n        ('id_step', ''),\n        ('user_sec, sys_sec', ''),\n        ('user_usec, sys_usec', ''),\n        ('tres_usage_in_max', ''),\n    ])\n\n    # Association table (centre-left, below job)\n    draw_table(0.5, 1.2, 4.5, 2, 'Association Table (create_assoc_table)', [\n        ('id_assoc', 'pk'),\n        ('user', ''),\n    ], color='#e8f5e9', border='#2e7d32')\n\n    # LDAP (right, below step)\n    draw_table(9, 1.2, 4.5, 2, 'LDAP / Active Directory', [\n        ('username  \\u2192  faculty (st attribute)', ''),\n    ], color='#fff3e0', border='#e65100')\n\n    # Arrow: Job -> Step (1:N)\n    ax.annotate('', xy=(9, 7), xytext=(5, 7),\n                arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n    ax.text(7, 7.3, '1 : N', ha='center', fontsize=10, color='#333')\n    ax.text(7, 6.7, 'job_db_inx', ha='center', fontsize=9, color='#666', style='italic')\n\n    # Arrow: Job -> Assoc (N:1)\n    ax.annotate('', xy=(2.75, 3.2), xytext=(2.75, 4),\n                arrowprops=dict(arrowstyle='->', color='#333', lw=1.5))\n    ax.text(3.0, 3.6, 'N : 1', ha='left', fontsize=10, color='#333')\n    ax.text(3.0, 3.25, 'id_assoc', ha='left', fontsize=9, color='#666', style='italic')\n\n    # Arrow: Assoc -> LDAP (dashed, lookup)\n    ax.annotate('', xy=(9, 2.2), xytext=(5, 2.2),\n                arrowprops=dict(arrowstyle='->', color='#666', lw=1.5, linestyle='dashed'))\n    ax.text(7, 2.5, 'username lookup', ha='center', fontsize=9, color='#666', style='italic')\n\n    plt.tight_layout()\n    plt.show()\n\ndraw_db_schema()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Raw Faculty Data\"))\n",
    "display(df_faculties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_exit_codes(exit_code_str):\n    if pd.isna(exit_code_str) or exit_code_str == '':\n        return {}\n    result = {}\n    for pair in str(exit_code_str).strip('\"').split(';'):\n        if ':' in pair:\n            code, count = pair.split(':')\n            result[int(code)] = int(count)\n    return result\n\ndisplay(Markdown(\"### Exit Code Analysis by Faculty\"))\n\nfor _, row in df_faculties.iterrows():\n    exit_codes = parse_exit_codes(row['exit_codes'])\n    if not exit_codes:\n        continue\n\n    sorted_codes = sorted(exit_codes.items(), key=lambda x: -x[1])\n    codes, counts = zip(*sorted_codes[:10])\n\n    fig, ax = plt.subplots(figsize=(10, 3))\n    colors = ['#4caf50' if c == 0 else '#f44336' for c in codes]\n    ax.bar([str(c) for c in codes], counts, color=colors, alpha=0.8)\n    ax.set_xlabel('Exit Code')\n    ax.set_ylabel('Count')\n    ax.set_title(f'Exit Code Distribution: {row[\"faculty\"]}')\n    plt.tight_layout()\n    plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}